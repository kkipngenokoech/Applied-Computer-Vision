{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Loading and Preprocessing in PyTorch for Image Classification\n",
    "\n",
    "This notebook covers:\n",
    "1. Understanding how datasets are loaded in PyTorch\n",
    "2. Applying image transformations and augmentations\n",
    "3. Understanding pointwise vs pixel-level transformations\n",
    "4. Handling datasets in different formats (folders, CSV, custom datasets)\n",
    "5. Exploring data dimensions and their effects on training\n",
    "6. Visualizing transformations and dataset properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Understanding Image Transformations\n",
    "Transformations in `torchvision.transforms` allow us to preprocess images before feeding them into a model. \n",
    "They can be categorized as:\n",
    "- **Pointwise transformations** (apply independently to each pixel, e.g., normalization, color jitter)\n",
    "- **Pixel-level transformations** (consider spatial relationships, e.g., rotations, flips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match pretrained model input\n",
    "    transforms.RandomHorizontalFlip(),  # Pixel-level transformation\n",
    "    transforms.RandomRotation(15),  # Pixel-level transformation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Pointwise transformation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pointwise transformation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "C:\\Users\\Emmanuel\\Desktop\\CMU\\Spring\\Assignments\\APPLIED COMPUTER VISION\\Assignment 1\\ACV_HW1\\ACV_LAB3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Loading Dataset from `torchvision`\n",
    "\n",
    "We use the `CIFAR-10` dataset, a commonly used dataset in vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define class labels\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 4. Exploring Data Dimensions\n",
    "Understanding how data dimensions affect training is crucial. \n",
    "For example, PyTorch expects image tensors in shape `(C, H, W)`, but sometimes datasets have `(H, W, C)` format.\n",
    "\"\"\"\n",
    "\n",
    "# Fetch a batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")  # Expected: (batch_size, 3, 224, 224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Different Dataset Formats\n",
    "Apart from `torchvision` datasets, real-world datasets may come in:\n",
    "- **Folder structure** (images inside labeled subdirectories)\n",
    "- **CSV files** (file paths with labels stored in a CSV file)\n",
    "- **Custom datasets** (image files with unknown format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Loading Images from Folder Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_dataset = datasets.ImageFolder(root='./custom_data', transform=transform)\n",
    "image_loader = DataLoader(image_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Loading Images from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "csv_dataset = CustomDataset(csv_file='./data/labels.csv', root_dir='./data/images', transform=transform)\n",
    "csv_loader = DataLoader(csv_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Image Transformations\n",
    "Let's visualize how transformations affect images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Display a batch of images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join(f'{classes[labels[j]]}' for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "- We explored different ways to load datasets in PyTorch.\n",
    "- We discussed pointwise and pixel-level transformations.\n",
    "- We handled different dataset formats (folders, CSV, custom loaders).\n",
    "- We examined data dimensions and their effects.\n",
    "- We visualized transformations to understand preprocessing better.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
